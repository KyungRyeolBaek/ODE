{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-09-15\n",
    "\n",
    "1. EC를 추가\n",
    "\n",
    "y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + EC*(x**5)\n",
    "\n",
    "2. 두가지 경우 비교\n",
    "\n",
    "y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + w5*(x**5)\n",
    "\n",
    "y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + EC*(x**5)\n",
    "\n",
    "y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + sigmoid* w *(x**5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-09-20\n",
    "\n",
    "loss =\n",
    "\n",
    "[f(1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4)) - {w1 + 2*w2 + 3*w3*(x**2) + 4*w4*(x**3)}\n",
    " + f(EC)*(x**5) - {5*f(EC)*(x**4) + d_EC*(x**5)}]**2\n",
    "\n",
    " [f(1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4)) - {w1 + 2*w2 + 3*w3*(x**2) + 4*w4*(x**3)}]**2\n",
    " + [f(EC)*(x**5) - {5*f(EC)*(x**4) + d_EC*(x**5)}]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-09-29\n",
    "\n",
    "Work 1\n",
    "\n",
    "1 : ODE 4차 오차 감소율 없을때 출력\n",
    "\n",
    "2 : ODE 5차 오차 감소율 없을때 출력\n",
    "\n",
    "3 : ODE 4차 + EC Case1 w 계수들 exact 계수들과 일치하는 경우에 대해서 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-10\n",
    "\n",
    "Exam 1\n",
    "\n",
    "exp(-x)\n",
    "\n",
    "Error, exact(4차, 5차), train(4차, 5차, 4차+EC Case1, Case2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-12\n",
    "\n",
    "EC 각 데이터별 학습\n",
    "\n",
    "내적 부분 변경\n",
    "\n",
    "데이터에 따라서 다른 값."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-16\n",
    "\n",
    "NN + NN*x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-23\n",
    "\n",
    "1_NN + 1_NN\n",
    "\n",
    "2_NN + 1_NN\n",
    "\n",
    "3_NN + 1_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-24\n",
    "\n",
    "코드 전체 다시 훑어 보기, 수정\n",
    "\n",
    "tensorflow2 함정\n",
    "\n",
    "range -> tf.range 사용하면 속도 빨라짐\n",
    "\n",
    "tensor로 계산되다 numpy 사용시 gradient 추척 못 할 수 있음\n",
    "\n",
    "tf.watch(변수)로 추적 해야 할 수 있음\n",
    "```\n",
    "ex)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(input_)\n",
    "    logits_seven = model(input_)[:, 7]\n",
    "grad_for_inp = tape.gradient(logits_seven, input_)\n",
    "\n",
    "print(grad_for_inp)\n",
    "```\n",
    "\n",
    "여러 손실을 총 손실에 추가하는 경우 추적을 못 할 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-26\n",
    "\n",
    "1 + NN(x)*x \n",
    "\n",
    "1 + w*x\n",
    "\n",
    "차이\n",
    "\n",
    "---\n",
    "\n",
    "loss_function2\n",
    "\n",
    "y'(x) = f(x, y)\n",
    "\n",
    "y(x) = p2(x) + z(x)\n",
    "\n",
    "z(x) = y(x) - p2(x) ~~ NN(x)*x := p2^\n",
    "\n",
    "z'(x) = f(x, y) - P2'(x) ~~ NN(x) + d_NN(x)*x\n",
    "\n",
    "Loss Function\n",
    "\n",
    "f(x, p2(x) + NN(x)*x) - p2'(x) - (NN(x) + d_NN(x)*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-27\n",
    "\n",
    "코드 정리\n",
    "\n",
    "loss_function 다양한 종류로 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-28\n",
    "\n",
    "loss Function2 \n",
    "\n",
    "f(x, p2(x) + NN(x)*x) - p2'(x) - (NN(x) + d_NN(x)*x)\n",
    "\n",
    "결과 좋음\n",
    "\n",
    "loss가 더 적은 결과가 exp와 더 멀어짐.\n",
    "\n",
    "loss 0.8664 일때 exp와 비슷함.\n",
    "\n",
    "근사값을 통해 loss를 구하기에 생기는 일 같다는 생각이 듦.\n",
    "\n",
    "오히려 loss가 0이 될수록 우리가 원하는 결과 값이랑 다를 수 있음.\n",
    "\n",
    "오히려 로컬 미니멈에 빠져야 원하는 결과를 얻을 수 있음.\n",
    "\n",
    "과적합이 문제가 됨\n",
    "\n",
    "random seed 값 알아 놓기\n",
    "\n",
    "f(x, p2(x) + NN(x)*x) - p2'(x) - (NN(x) + d_NN(x)*x)\n",
    "\n",
    "bias 추가 하면 어떠할지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-29\n",
    "\n",
    "bias 추가 -> 결과 안좋음\n",
    "\n",
    "1 + EC random seed 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    모든 주석은 읽고 필요 없는 부분은 지우셔도 됩니다.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    모든 주석은 읽고 필요 없는 부분은 지우셔도 됩니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow import transpose\n",
    "from tensorflow.math import sigmoid\n",
    "from tensorflow.experimental.numpy import dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the following initial value differential equation $$ y'=\\lambda y,  \\quad  y(0)=y_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuction Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psy_analytic(x, lamb = 1):\n",
    "    '''\n",
    "        psy_analytic = np.exp(lamb*x)\n",
    "    '''\n",
    "    return tf.math.exp(lamb*x)\n",
    "\n",
    "def A(x):\n",
    "    return -1\n",
    "\n",
    "def B(x):\n",
    "    return 0\n",
    "\n",
    "def f(x, psy):\n",
    "    '''\n",
    "        d(psy)/dx = f(x, psy)\n",
    "        y' = B(x) - A(x) * y\n",
    "    '''\n",
    "    return B(x) - A(x)*psy\n",
    "\n",
    "def y(x, w, n = 4, bias = 1.): \n",
    "    '''\n",
    "        y(x) = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + ... + wn*(x**n)\n",
    "    '''\n",
    "    y = bias\n",
    "    for i in range(1, n+1):\n",
    "        y += w[i-1]*x**i\n",
    "    \n",
    "    return y\n",
    "\n",
    "def yprime(x, w, n=4, bias = 0.):\n",
    "    '''\n",
    "        yprime = w1 + 2*w2*x + 3*w3*(x**2) + 4*w4*(x**3) + ... + n*wn*(x**(n-1))\n",
    "    '''\n",
    "    yprime = bias\n",
    "    for i in range(n):\n",
    "        yprime += (i+1)*w[i]*x**i\n",
    "    \n",
    "    return yprime\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def neural_network(W, x):\n",
    "    a1 = sigmoid(dot(x, W[0]))\n",
    "    return dot(a1, W[1])[0][0]\n",
    "\n",
    "def d_neural_network_dx(W, x, k=1):\n",
    "    return dot(dot(transpose(W[1]), transpose(W[0])**k), sigmoid_grad(x))[0][0]\n",
    "\n",
    "def loss_function_yhat(w, x, n):\n",
    "    err_sqr = tf.square(yprime(x, w, n, bias = 0) - f(x, y(x, w, n, bias = 1)))\n",
    "    loss_sum = tf.math.reduce_sum(err_sqr)\n",
    "    return loss_sum\n",
    "\n",
    "def loss_function(yhat, yhatprime, W, x):\n",
    "    loss_sum = 0.\n",
    "    for i in tf.range(tf.size(x)):\n",
    "        a1 = sigmoid(dot(x[i], W[0]))\n",
    "        net_out = dot(a1, W[1])\n",
    "        psy_t = yhat[i] + net_out*x[i]\n",
    "\n",
    "        d_net_out = dot(dot(transpose(W[1]), transpose(W[0])**1), sigmoid_grad(x[i]))\n",
    "        d_psy_t = yhatprime[i] + net_out + d_net_out*x[i]\n",
    "        \n",
    "        err_sqr = tf.square(d_net_out - f(x[i], psy_t))\n",
    "        loss_sum += tf.reduce_sum(err_sqr)\n",
    "        \n",
    "    return loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    yhat_w_loss 출력\n",
    "    w 값, low_loss_sum(가장 낮은 지점 loss 합)\n",
    "\n",
    "    yhat 값 생성 방법\n",
    "    yhat_w, yhat_loss = yhat_w_loss(x, 차수)\n",
    "    yhat = y(x, yhat_w, 차수, bias)\n",
    "'''\n",
    "def yhat_w_loss(x, n = 4, target_loss = np.Inf, early_stop = np.Inf, max_train_count = np.Inf):\n",
    "    print('Start yhat')\n",
    "    # 차수, weight 개수\n",
    "    w = tf.Variable(tf.random.normal(shape=(n,), dtype=tf.float32, seed=777), trainable=True) # w = [weight1, weight2, weight3, weight4, ... , weight(n)]\n",
    "\n",
    "    # 원하는 loss 값에 도달할 때 까지 Epoch 횟수 올리기\n",
    "    Epoch = 0\n",
    "\n",
    "    # 옵티마이져 종류 Adam, SDG, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl 가능\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    # 학습 과정\n",
    "    low_loss = np.inf\n",
    "    low_loss_Epoch = 0\n",
    "    while True:\n",
    "        Epoch += 1\n",
    "        loss_sum = 0.0\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_sum = loss_function_yhat(w, x, n)\n",
    "\n",
    "        # Gradient 값 저장\n",
    "        gradients = tape.gradient(loss_sum, w)\n",
    "        # Gradient 값 w에 적용\n",
    "        optimizer.apply_gradients(zip([gradients], [w]))\n",
    "        \n",
    "        # early stop\n",
    "        '''\n",
    "            target_loss : 초기값 inf, 값을 정해 줄 수 있음.\n",
    "            low_loss : 반복중 가장 낮은 loss 값\n",
    "            early_stop : 초기값 inf, low_loss를 구하고 얼마나 반복 하고 멈출 것인지.\n",
    "            조건\n",
    "            1. target_loss 보다 loss_sum이 작아졌는지\n",
    "            2. low_loss 보다 loss_sum이 작아졌는지\n",
    "\n",
    "            조건 1, 2 만족 시 low_loss가 loss_sum으로 변경\n",
    "\n",
    "            3. low_loss가 갱신된 Epoch 부터 지정한 early_stop 값 만큼 반복 후 중단.\n",
    "        '''\n",
    "        if target_loss >= loss_sum and low_loss > loss_sum:\n",
    "            low_loss = loss_sum\n",
    "            early_stop_w = w\n",
    "            low_loss_Epoch = Epoch\n",
    "        \n",
    "        if low_loss_Epoch != 0 and low_loss_Epoch + early_stop <= Epoch:\n",
    "            print('early stop')\n",
    "            break\n",
    "\n",
    "        # 학습 중간에 출력\n",
    "        if Epoch % 1000 == 0:\n",
    "            print('Epoch : {}, Loss_sum : {:.4f}, w : {}'.format(Epoch, loss_sum, w.numpy()))\n",
    "\n",
    "        if Epoch == max_train_count:\n",
    "            print('max count')\n",
    "            break\n",
    "        \n",
    "    print('low_loss_Epoch : {}, low_Loss_sum : {:.4f}, low_loss_w : {}'.format(low_loss_Epoch, low_loss, early_stop_w))\n",
    "\n",
    "    return early_stop_w, low_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EC_W(x, yhat, yhatprime, n = 4, min_loss = -1, target_loss = np.Inf, early_stop = np.Inf, max_train_count = np.Inf):\n",
    "    print('Start add EC')\n",
    "    seed = 777\n",
    "    W = [tf.Variable(tf.random.normal(shape=(1, 10), dtype=tf.float32, seed=seed), trainable=True), \n",
    "        tf.Variable(tf.random.normal(shape=(10, 1), dtype=tf.float32, seed=seed), trainable=True)] \n",
    "    \n",
    "    # 원하는 loss 값에 도달할 때 까지 Epoch 횟수 올리기\n",
    "    Epoch = 0\n",
    "\n",
    "    # 옵티마이져 종류 Adam, SDG, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl 가능\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    # 학습 과정\n",
    "    low_loss = np.inf\n",
    "    low_loss_Epoch = 0\n",
    "    while True:\n",
    "        loss_sum = 0.0\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_sum = loss_function(yhat, yhatprime, W, x)\n",
    "\n",
    "        # Gradient 값 저장\n",
    "        gradients = tape.gradient(loss_sum, W)\n",
    "        # Gradient 값 w에 적용\n",
    "        optimizer.apply_gradients(zip([gradients[0], gradients[1]], [W[0], W[1]]))\n",
    "\n",
    "        # early stop\n",
    "        if target_loss >= loss_sum and low_loss > loss_sum:\n",
    "            low_loss = loss_sum\n",
    "            early_stop_w = W\n",
    "            low_loss_Epoch = Epoch\n",
    "\n",
    "        if loss_sum <= 0.5:\n",
    "            print('not this seed : {}, break Epoch {}'.format(seed, Epoch))\n",
    "            seed += 1\n",
    "            Epoch = 0\n",
    "            low_loss = np.inf\n",
    "            low_loss_Epoch = 0\n",
    "            W = [tf.Variable(tf.random.normal(shape=(1, 10), dtype=tf.float32, seed=seed), trainable=True), \n",
    "                tf.Variable(tf.random.normal(shape=(10, 1), dtype=tf.float32, seed=seed), trainable=True)] \n",
    "            continue\n",
    "        \n",
    "        if low_loss_Epoch != 0 and low_loss_Epoch + early_stop <= Epoch:\n",
    "            print('early stop')\n",
    "            break\n",
    "\n",
    "        # min_loss가 loss_sum 보다 작아지면 멈춤\n",
    "        if min_loss > loss_sum:\n",
    "            print('min_loss')\n",
    "            break\n",
    "\n",
    "        if Epoch % 1000 == 0:\n",
    "            print('Epoch : {}, Loss_sum : {:.4f}'.format(Epoch, loss_sum))\n",
    "\n",
    "        if Epoch == max_train_count:\n",
    "            print('max count')\n",
    "            break\n",
    "        Epoch += 1\n",
    "    print('low_loss_Epoch : {}, low_Loss_sum : {:.4f}, low_loss_w : {}, seed : {}'.format(low_loss_Epoch, low_loss, early_stop_w, seed))\n",
    "\n",
    "    return early_stop_w, low_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start yhat\n",
      "Epoch : 1000, Loss_sum : 24.7032, w : [1.4452205]\n",
      "early stop\n",
      "low_loss_Epoch : 1582, low_Loss_sum : 24.6271, low_loss_w : <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.4896877], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# random seed 값 지정, global한 randomseed값과 operation seed를 모두 설정해야 같은 결과값을 도출할 수 있음.\n",
    "tf.random.set_seed(7777)\n",
    "\n",
    "# 최대 x 범위 : start <= x < limit, delta로 슬라이싱\n",
    "start, limit, delta = 0, 1, 0.01\n",
    "x = tf.Variable(tf.range(start, limit, delta))\n",
    "\n",
    "# yhat 생성\n",
    "n = 1\n",
    "yhat_w_1, low_loss_1 = yhat_w_loss(x, n, early_stop=1)\n",
    "yhat_1 = y(x, yhat_w_1, n, bias = 1)\n",
    "yhatprime_1 = yprime(x, yhat_w_1, n, bias = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start add EC\n",
      "Epoch : 0, Loss_sum : 89.7498\n",
      "not this seed : 777, break Epoch 649\n",
      "Epoch : 0, Loss_sum : 948.3143\n",
      "Epoch : 1000, Loss_sum : 10.4575\n",
      "not this seed : 778, break Epoch 1836\n",
      "Epoch : 0, Loss_sum : 196.6952\n",
      "Epoch : 1000, Loss_sum : 0.5046\n",
      "Epoch : 2000, Loss_sum : 0.5036\n",
      "Epoch : 3000, Loss_sum : 0.5016\n",
      "not this seed : 779, break Epoch 3462\n",
      "Epoch : 0, Loss_sum : 156.7578\n",
      "not this seed : 780, break Epoch 199\n",
      "Epoch : 0, Loss_sum : 394.1915\n",
      "not this seed : 781, break Epoch 254\n",
      "Epoch : 0, Loss_sum : 275.5311\n",
      "not this seed : 782, break Epoch 249\n",
      "Epoch : 0, Loss_sum : 707.3459\n",
      "Epoch : 1000, Loss_sum : 1.1221\n",
      "not this seed : 783, break Epoch 1207\n",
      "Epoch : 0, Loss_sum : 170.7715\n",
      "not this seed : 784, break Epoch 218\n",
      "Epoch : 0, Loss_sum : 1161.9353\n",
      "Epoch : 1000, Loss_sum : 3.4578\n",
      "not this seed : 785, break Epoch 1347\n",
      "Epoch : 0, Loss_sum : 133.8578\n",
      "not this seed : 786, break Epoch 220\n",
      "Epoch : 0, Loss_sum : 78.0702\n",
      "not this seed : 787, break Epoch 83\n",
      "Epoch : 0, Loss_sum : 299.7001\n",
      "not this seed : 788, break Epoch 406\n",
      "Epoch : 0, Loss_sum : 61.8973\n",
      "not this seed : 789, break Epoch 166\n",
      "Epoch : 0, Loss_sum : 1568.1636\n",
      "not this seed : 790, break Epoch 974\n",
      "Epoch : 0, Loss_sum : 160.6098\n",
      "not this seed : 791, break Epoch 136\n",
      "Epoch : 0, Loss_sum : 458.5810\n",
      "not this seed : 792, break Epoch 481\n",
      "Epoch : 0, Loss_sum : 72.2281\n",
      "not this seed : 793, break Epoch 85\n",
      "Epoch : 0, Loss_sum : 22.6965\n",
      "not this seed : 794, break Epoch 40\n",
      "Epoch : 0, Loss_sum : 706.3495\n",
      "not this seed : 795, break Epoch 441\n",
      "Epoch : 0, Loss_sum : 22.1899\n",
      "not this seed : 796, break Epoch 48\n",
      "Epoch : 0, Loss_sum : 809.6699\n",
      "not this seed : 797, break Epoch 512\n",
      "Epoch : 0, Loss_sum : 1369.2598\n",
      "Epoch : 1000, Loss_sum : 0.7639\n",
      "not this seed : 798, break Epoch 1095\n",
      "Epoch : 0, Loss_sum : 35.7669\n",
      "not this seed : 799, break Epoch 39\n",
      "Epoch : 0, Loss_sum : 705.9102\n",
      "not this seed : 800, break Epoch 854\n",
      "Epoch : 0, Loss_sum : 515.2693\n",
      "not this seed : 801, break Epoch 637\n",
      "Epoch : 0, Loss_sum : 337.5840\n",
      "not this seed : 802, break Epoch 571\n",
      "Epoch : 0, Loss_sum : 79.3283\n",
      "not this seed : 803, break Epoch 287\n",
      "Epoch : 0, Loss_sum : 31.6097\n",
      "not this seed : 804, break Epoch 83\n",
      "Epoch : 0, Loss_sum : 494.2562\n",
      "not this seed : 805, break Epoch 461\n",
      "Epoch : 0, Loss_sum : 209.3267\n",
      "not this seed : 806, break Epoch 291\n",
      "Epoch : 0, Loss_sum : 90.9435\n",
      "not this seed : 807, break Epoch 85\n",
      "Epoch : 0, Loss_sum : 475.4918\n",
      "not this seed : 808, break Epoch 471\n",
      "Epoch : 0, Loss_sum : 349.2991\n",
      "not this seed : 809, break Epoch 416\n",
      "Epoch : 0, Loss_sum : 552.1380\n",
      "not this seed : 810, break Epoch 690\n",
      "Epoch : 0, Loss_sum : 363.5651\n",
      "not this seed : 811, break Epoch 229\n",
      "Epoch : 0, Loss_sum : 165.3387\n",
      "not this seed : 812, break Epoch 97\n",
      "Epoch : 0, Loss_sum : 71.2537\n",
      "not this seed : 813, break Epoch 53\n",
      "Epoch : 0, Loss_sum : 852.1179\n",
      "not this seed : 814, break Epoch 636\n",
      "Epoch : 0, Loss_sum : 594.6232\n",
      "Epoch : 1000, Loss_sum : 0.5144\n",
      "not this seed : 815, break Epoch 1046\n",
      "Epoch : 0, Loss_sum : 719.9579\n",
      "Epoch : 1000, Loss_sum : 0.6934\n",
      "not this seed : 816, break Epoch 1099\n",
      "Epoch : 0, Loss_sum : 69.2210\n",
      "not this seed : 817, break Epoch 50\n",
      "Epoch : 0, Loss_sum : 267.5902\n",
      "not this seed : 818, break Epoch 440\n",
      "Epoch : 0, Loss_sum : 820.1174\n",
      "Epoch : 1000, Loss_sum : 0.6280\n",
      "not this seed : 819, break Epoch 1035\n",
      "Epoch : 0, Loss_sum : 5.9588\n",
      "not this seed : 820, break Epoch 10\n",
      "Epoch : 0, Loss_sum : 535.3764\n",
      "not this seed : 821, break Epoch 571\n",
      "Epoch : 0, Loss_sum : 797.2945\n",
      "not this seed : 822, break Epoch 774\n",
      "Epoch : 0, Loss_sum : 73.1835\n",
      "not this seed : 823, break Epoch 192\n",
      "Epoch : 0, Loss_sum : 4.2366\n",
      "not this seed : 824, break Epoch 16\n",
      "Epoch : 0, Loss_sum : 200.9034\n",
      "not this seed : 825, break Epoch 339\n",
      "Epoch : 0, Loss_sum : 185.9669\n",
      "not this seed : 826, break Epoch 283\n",
      "Epoch : 0, Loss_sum : 292.6058\n",
      "not this seed : 827, break Epoch 194\n",
      "Epoch : 0, Loss_sum : 1.2573\n",
      "not this seed : 828, break Epoch 11\n",
      "Epoch : 0, Loss_sum : 23.2334\n",
      "not this seed : 829, break Epoch 25\n",
      "Epoch : 0, Loss_sum : 253.0540\n",
      "not this seed : 830, break Epoch 270\n",
      "Epoch : 0, Loss_sum : 609.2573\n",
      "not this seed : 831, break Epoch 342\n",
      "Epoch : 0, Loss_sum : 1080.7274\n",
      "Epoch : 1000, Loss_sum : 2.7265\n",
      "not this seed : 832, break Epoch 1394\n",
      "Epoch : 0, Loss_sum : 299.2257\n",
      "not this seed : 833, break Epoch 410\n",
      "Epoch : 0, Loss_sum : 38.0352\n",
      "not this seed : 834, break Epoch 90\n",
      "Epoch : 0, Loss_sum : 686.2638\n",
      "not this seed : 835, break Epoch 507\n",
      "Epoch : 0, Loss_sum : 208.5999\n",
      "not this seed : 836, break Epoch 245\n",
      "Epoch : 0, Loss_sum : 557.5988\n",
      "not this seed : 837, break Epoch 442\n",
      "Epoch : 0, Loss_sum : 104.0974\n",
      "not this seed : 838, break Epoch 254\n",
      "Epoch : 0, Loss_sum : 40.5804\n",
      "not this seed : 839, break Epoch 216\n",
      "Epoch : 0, Loss_sum : 334.7349\n",
      "not this seed : 840, break Epoch 552\n",
      "Epoch : 0, Loss_sum : 101.6257\n",
      "not this seed : 841, break Epoch 232\n",
      "Epoch : 0, Loss_sum : 503.8058\n",
      "not this seed : 842, break Epoch 623\n",
      "Epoch : 0, Loss_sum : 483.6288\n",
      "not this seed : 843, break Epoch 517\n",
      "Epoch : 0, Loss_sum : 87.1819\n",
      "not this seed : 844, break Epoch 159\n",
      "Epoch : 0, Loss_sum : 50.9944\n",
      "not this seed : 845, break Epoch 146\n",
      "Epoch : 0, Loss_sum : 381.4808\n",
      "not this seed : 846, break Epoch 313\n",
      "Epoch : 0, Loss_sum : 1248.0269\n",
      "Epoch : 1000, Loss_sum : 2.9874\n",
      "not this seed : 847, break Epoch 1591\n",
      "Epoch : 0, Loss_sum : 545.0015\n",
      "not this seed : 848, break Epoch 540\n",
      "Epoch : 0, Loss_sum : 477.1920\n",
      "not this seed : 849, break Epoch 263\n",
      "Epoch : 0, Loss_sum : 194.7946\n",
      "not this seed : 850, break Epoch 377\n",
      "Epoch : 0, Loss_sum : 727.8068\n",
      "not this seed : 851, break Epoch 786\n",
      "Epoch : 0, Loss_sum : 679.8687\n",
      "not this seed : 852, break Epoch 601\n",
      "Epoch : 0, Loss_sum : 274.6770\n",
      "not this seed : 853, break Epoch 585\n",
      "Epoch : 0, Loss_sum : 712.4752\n",
      "not this seed : 854, break Epoch 569\n",
      "Epoch : 0, Loss_sum : 102.8622\n",
      "not this seed : 855, break Epoch 71\n",
      "Epoch : 0, Loss_sum : 356.0750\n",
      "not this seed : 856, break Epoch 216\n",
      "Epoch : 0, Loss_sum : 38.3471\n",
      "not this seed : 857, break Epoch 49\n",
      "Epoch : 0, Loss_sum : 524.7764\n",
      "not this seed : 858, break Epoch 929\n",
      "Epoch : 0, Loss_sum : 37.9215\n",
      "not this seed : 859, break Epoch 103\n",
      "Epoch : 0, Loss_sum : 68.6992\n",
      "not this seed : 860, break Epoch 35\n",
      "Epoch : 0, Loss_sum : 512.3391\n",
      "not this seed : 861, break Epoch 669\n",
      "Epoch : 0, Loss_sum : 258.4605\n"
     ]
    }
   ],
   "source": [
    "# add EC\n",
    "EC_W_1, loss_1 = EC_W(x, yhat_1, yhatprime_1, n, early_stop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "fig = plt.figure(figsize=(18,5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(yhat_1, label = 'yhat')\n",
    "ax.plot(yhat_1 + [neural_network(EC_W_1, x[i])*x[i] for i in tf.range(tf.size(x))], label = 'y')\n",
    "ax.plot(psy_analytic(x,1), label = 'analysis')\n",
    "ax.tick_params(labelsize=10)\n",
    "ax.legend(fontsize=20)\n",
    "ax.grid()\n",
    "# Error\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(abs(yhat_1 - psy_analytic(x,1)), label = 'yhat')\n",
    "ax2.plot(abs(yhat_1 + [neural_network(EC_W_1, x[i])*x[i] for i in tf.range(tf.size(x))] - psy_analytic(x,1)), label = 'y')\n",
    "ax2.tick_params(labelsize=10)\n",
    "ax2.legend(fontsize=20)\n",
    "ax2.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ODE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a37a875133dc80c33f2ec96aea7a9fa5375ebe8d8c5e67883faafd638e38fe9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
