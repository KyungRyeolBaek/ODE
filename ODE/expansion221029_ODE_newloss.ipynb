{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-09-15\n",
    "\n",
    "1. EC를 추가\n",
    "\n",
    "y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + EC*(x**5)\n",
    "\n",
    "2. 두가지 경우 비교\n",
    "\n",
    "y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + w5*(x**5)\n",
    "\n",
    "y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + EC*(x**5)\n",
    "\n",
    "y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + sigmoid* w *(x**5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-09-20\n",
    "\n",
    "loss =\n",
    "\n",
    "[f(1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4)) - {w1 + 2*w2 + 3*w3*(x**2) + 4*w4*(x**3)}\n",
    " + f(EC)*(x**5) - {5*f(EC)*(x**4) + d_EC*(x**5)}]**2\n",
    "\n",
    " [f(1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4)) - {w1 + 2*w2 + 3*w3*(x**2) + 4*w4*(x**3)}]**2\n",
    " + [f(EC)*(x**5) - {5*f(EC)*(x**4) + d_EC*(x**5)}]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-09-29\n",
    "\n",
    "Work 1\n",
    "\n",
    "1 : ODE 4차 오차 감소율 없을때 출력\n",
    "\n",
    "2 : ODE 5차 오차 감소율 없을때 출력\n",
    "\n",
    "3 : ODE 4차 + EC Case1 w 계수들 exact 계수들과 일치하는 경우에 대해서 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-10\n",
    "\n",
    "Exam 1\n",
    "\n",
    "exp(-x)\n",
    "\n",
    "Error, exact(4차, 5차), train(4차, 5차, 4차+EC Case1, Case2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-12\n",
    "\n",
    "EC 각 데이터별 학습\n",
    "\n",
    "내적 부분 변경\n",
    "\n",
    "데이터에 따라서 다른 값."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-16\n",
    "\n",
    "NN + NN*x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-23\n",
    "\n",
    "1_NN + 1_NN\n",
    "\n",
    "2_NN + 1_NN\n",
    "\n",
    "3_NN + 1_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-24\n",
    "\n",
    "코드 전체 다시 훑어 보기, 수정\n",
    "\n",
    "tensorflow2 함정\n",
    "\n",
    "range -> tf.range 사용하면 속도 빨라짐\n",
    "\n",
    "tensor로 계산되다 numpy 사용시 gradient 추척 못 할 수 있음\n",
    "\n",
    "tf.watch(변수)로 추적 해야 할 수 있음\n",
    "```\n",
    "ex)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(input_)\n",
    "    logits_seven = model(input_)[:, 7]\n",
    "grad_for_inp = tape.gradient(logits_seven, input_)\n",
    "\n",
    "print(grad_for_inp)\n",
    "```\n",
    "\n",
    "여러 손실을 총 손실에 추가하는 경우 추적을 못 할 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-26\n",
    "\n",
    "1 + NN(x)*x \n",
    "\n",
    "1 + w*x\n",
    "\n",
    "차이\n",
    "\n",
    "---\n",
    "\n",
    "loss_function2\n",
    "\n",
    "y'(x) = f(x, y)\n",
    "\n",
    "y(x) = p2(x) + z(x)\n",
    "\n",
    "z(x) = y(x) - p2(x) ~~ NN(x)*x := p2^\n",
    "\n",
    "z'(x) = f(x, y) - P2'(x) ~~ NN(x) + d_NN(x)*x\n",
    "\n",
    "Loss Function\n",
    "\n",
    "f(x, p2(x) + NN(x)*x) - p2'(x) - (NN(x) + d_NN(x)*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-27\n",
    "\n",
    "코드 정리\n",
    "\n",
    "loss_function 다양한 종류로 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-28\n",
    "\n",
    "loss Function2 \n",
    "\n",
    "f(x, p2(x) + NN(x)*x) - p2'(x) - (NN(x) + d_NN(x)*x)\n",
    "\n",
    "결과 좋음\n",
    "\n",
    "loss가 더 적은 결과가 exp와 더 멀어짐.\n",
    "\n",
    "loss 0.8664 일때 exp와 비슷함.\n",
    "\n",
    "근사값을 통해 loss를 구하기에 생기는 일 같다는 생각이 듦.\n",
    "\n",
    "오히려 loss가 0이 될수록 우리가 원하는 결과 값이랑 다를 수 있음.\n",
    "\n",
    "오히려 로컬 미니멈에 빠져야 원하는 결과를 얻을 수 있음.\n",
    "\n",
    "과적합이 문제가 됨\n",
    "\n",
    "random seed 값 알아 놓기\n",
    "\n",
    "f(x, p2(x) + NN(x)*x) - p2'(x) - (NN(x) + d_NN(x)*x)\n",
    "\n",
    "bias 추가 하면 어떠할지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-10-29\n",
    "\n",
    "bias 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    모든 주석은 읽고 필요 없는 부분은 지우셔도 됩니다.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    모든 주석은 읽고 필요 없는 부분은 지우셔도 됩니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow import transpose\n",
    "from tensorflow.math import sigmoid\n",
    "from tensorflow.experimental.numpy import dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the following initial value differential equation $$ y'=\\lambda y,  \\quad  y(0)=y_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuction Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psy_analytic(x, lamb = 1):\n",
    "    '''\n",
    "        psy_analytic = np.exp(lamb*x)\n",
    "    '''\n",
    "    return tf.math.exp(lamb*x)\n",
    "\n",
    "def A(x):\n",
    "    return -1\n",
    "\n",
    "def B(x):\n",
    "    return 0\n",
    "\n",
    "def f(x, psy):\n",
    "    '''\n",
    "        d(psy)/dx = f(x, psy)\n",
    "        y' = B(x) - A(x) * y\n",
    "    '''\n",
    "    return B(x) - A(x)*psy\n",
    "\n",
    "def y(x, w, n = 4, bias = 1.): \n",
    "    '''\n",
    "        y(x) = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + ... + wn*(x**n)\n",
    "    '''\n",
    "    y = bias\n",
    "    for i in range(1, n+1):\n",
    "        y += w[i-1]*x**i\n",
    "    \n",
    "    return y\n",
    "\n",
    "def yprime(x, w, n=4, bias = 0.):\n",
    "    '''\n",
    "        yprime = w1 + 2*w2*x + 3*w3*(x**2) + 4*w4*(x**3) + ... + n*wn*(x**(n-1))\n",
    "    '''\n",
    "    yprime = bias\n",
    "    for i in range(n):\n",
    "        yprime += (i+1)*w[i]*x**i\n",
    "    \n",
    "    return yprime\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def neural_network(W, x):\n",
    "    a1 = sigmoid(dot(x, W[0]))\n",
    "    return dot(a1, W[1])[0][0]\n",
    "\n",
    "def d_neural_network_dx(W, x, k=1):\n",
    "    return dot(dot(transpose(W[1]), transpose(W[0])**k), sigmoid_grad(x))[0][0]\n",
    "\n",
    "def loss_function_yhat(w, x, n):\n",
    "    err_sqr = tf.square(yprime(x, w, n, bias = 0) - f(x, y(x, w, n, bias = 1)))\n",
    "    loss_sum = tf.math.reduce_sum(err_sqr)\n",
    "    return loss_sum\n",
    "\n",
    "def loss_function(yhat, yhatprime, W, b, x, n):\n",
    "    loss_sum = 0.\n",
    "    for i in tf.range(tf.size(x)):\n",
    "        a1 = sigmoid(dot(x[i], W[0]))\n",
    "        net_out = dot(a1, W[1])\n",
    "        psy_t = yhat[i] + b + net_out*x[i]**(n+1)\n",
    "\n",
    "        d_net_out = dot(dot(transpose(W[1]), transpose(W[0])**1), sigmoid_grad(x[i]))\n",
    "        d_psy_t = yhatprime[i] + (n+1)*net_out*x[i]**n + d_net_out*x[i]**(n+1)\n",
    "        \n",
    "        err_sqr = tf.square(d_net_out - f(x[i], psy_t))\n",
    "        loss_sum += tf.reduce_sum(err_sqr)\n",
    "        \n",
    "    return loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    yhat_w_loss 출력\n",
    "    w 값, low_loss_sum(가장 낮은 지점 loss 합)\n",
    "\n",
    "    yhat 값 생성 방법\n",
    "    yhat_w, yhat_loss = yhat_w_loss(x, 차수)\n",
    "    yhat = y(x, yhat_w, 차수, bias)\n",
    "'''\n",
    "def yhat_w_loss(x, n = 4, target_loss = np.Inf, early_stop = np.Inf, max_train_count = np.Inf):\n",
    "    print('Start yhat')\n",
    "    # 차수, weight 개수\n",
    "    w = tf.Variable(tf.random.normal(shape=(n,), dtype=tf.float32, seed=777), trainable=True) # w = [weight1, weight2, weight3, weight4, ... , weight(n)]\n",
    "\n",
    "    # 원하는 loss 값에 도달할 때 까지 Epoch 횟수 올리기\n",
    "    Epoch = 0\n",
    "\n",
    "    # 옵티마이져 종류 Adam, SDG, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl 가능\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    # 학습 과정\n",
    "    low_loss = np.inf\n",
    "    low_loss_Epoch = 0\n",
    "    while True:\n",
    "        Epoch += 1\n",
    "        loss_sum = 0.0\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_sum = loss_function_yhat(w, x, n)\n",
    "\n",
    "        # Gradient 값 저장\n",
    "        gradients = tape.gradient(loss_sum, w)\n",
    "        # Gradient 값 w에 적용\n",
    "        optimizer.apply_gradients(zip([gradients], [w]))\n",
    "        \n",
    "        # early stop\n",
    "        '''\n",
    "            target_loss : 초기값 inf, 값을 정해 줄 수 있음.\n",
    "            low_loss : 반복중 가장 낮은 loss 값\n",
    "            early_stop : 초기값 inf, low_loss를 구하고 얼마나 반복 하고 멈출 것인지.\n",
    "            조건\n",
    "            1. target_loss 보다 loss_sum이 작아졌는지\n",
    "            2. low_loss 보다 loss_sum이 작아졌는지\n",
    "\n",
    "            조건 1, 2 만족 시 low_loss가 loss_sum으로 변경\n",
    "\n",
    "            3. low_loss가 갱신된 Epoch 부터 지정한 early_stop 값 만큼 반복 후 중단.\n",
    "        '''\n",
    "        if target_loss >= loss_sum and low_loss > loss_sum:\n",
    "            low_loss = loss_sum\n",
    "            early_stop_w = w\n",
    "            low_loss_Epoch = Epoch\n",
    "        \n",
    "        if low_loss_Epoch != 0 and low_loss_Epoch + early_stop <= Epoch:\n",
    "            print('early stop')\n",
    "            break\n",
    "\n",
    "        # 학습 중간에 출력\n",
    "        if Epoch % 1000 == 0:\n",
    "            print('Epoch : {}, Loss_sum : {:.4f}, w : {}'.format(Epoch, loss_sum, w.numpy()))\n",
    "\n",
    "        if Epoch == max_train_count:\n",
    "            print('max count')\n",
    "            break\n",
    "        \n",
    "    print('low_loss_Epoch : {}, low_Loss_sum : {:.4f}, low_loss_w : {}'.format(low_loss_Epoch, low_loss, early_stop_w))\n",
    "\n",
    "    return early_stop_w, low_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EC_W(x, yhat, yhatprime, n = 4, min_loss = -1, target_loss = np.Inf, early_stop = np.Inf, max_train_count = np.Inf):\n",
    "    print('Start add EC')\n",
    "    seed = 777\n",
    "    b = tf.Variable(tf.random.normal(shape=(1,), dtype=tf.float32, seed=seed), trainable=True)\n",
    "    W = [tf.Variable(tf.random.normal(shape=(1, 10), dtype=tf.float32, seed=seed), trainable=True), \n",
    "        tf.Variable(tf.random.normal(shape=(10, 1), dtype=tf.float32, seed=seed), trainable=True)] \n",
    "    \n",
    "    # 원하는 loss 값에 도달할 때 까지 Epoch 횟수 올리기\n",
    "    Epoch = 0\n",
    "\n",
    "    # 옵티마이져 종류 Adam, SDG, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl 가능\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    # 학습 과정\n",
    "    low_loss = np.inf\n",
    "    low_loss_Epoch = 0\n",
    "    while True:\n",
    "        loss_sum = 0.0\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_sum = loss_function(yhat, yhatprime, W, b, x, n)\n",
    "\n",
    "        # Gradient 값 저장\n",
    "        gradients = tape.gradient(loss_sum, [*W, b])\n",
    "        # Gradient 값 w에 적용\n",
    "        optimizer.apply_gradients(zip([gradients[0], gradients[1], gradients[2]], [W[0], W[1], b]))\n",
    "\n",
    "        # early stop\n",
    "        if target_loss >= loss_sum and low_loss > loss_sum:\n",
    "            low_loss = loss_sum\n",
    "            early_stop_w, early_stop_b = W, b\n",
    "            low_loss_Epoch = Epoch\n",
    "\n",
    "        if loss_sum <= 0.8:\n",
    "            print('not this seed : {}, break Epoch {}'.format(seed, Epoch))\n",
    "            seed += 1\n",
    "            Epoch = 0\n",
    "            low_loss = np.inf\n",
    "            low_loss_Epoch = 0\n",
    "            b = tf.Variable(tf.random.normal(shape=(1,), dtype=tf.float32, seed=seed), trainable=True)\n",
    "            W = [tf.Variable(tf.random.normal(shape=(1, 10), dtype=tf.float32, seed=seed), trainable=True), \n",
    "                tf.Variable(tf.random.normal(shape=(10, 1), dtype=tf.float32, seed=seed), trainable=True)] \n",
    "            continue\n",
    "        \n",
    "        if low_loss_Epoch != 0 and low_loss_Epoch + early_stop <= Epoch:\n",
    "            print('early stop')\n",
    "            break\n",
    "\n",
    "        # min_loss가 loss_sum 보다 작아지면 멈춤\n",
    "        if min_loss > loss_sum:\n",
    "            print('min_loss')\n",
    "            break\n",
    "\n",
    "        if Epoch % 1000 == 0:\n",
    "            print('Epoch : {}, Loss_sum : {:.4f}'.format(Epoch, loss_sum))\n",
    "\n",
    "        if Epoch == max_train_count:\n",
    "            print('max count')\n",
    "            break\n",
    "        Epoch += 1\n",
    "    print('low_loss_Epoch : {}, low_Loss_sum : {:.4f}, low_loss_w : {}, seed : {}'.format(low_loss_Epoch, low_loss, early_stop_w, seed))\n",
    "\n",
    "    return early_stop_w, early_stop_b, low_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start yhat\n",
      "Epoch : 1000, Loss_sum : 24.7032, w : [1.4452205]\n",
      "early stop\n",
      "low_loss_Epoch : 1582, low_Loss_sum : 24.6271, low_loss_w : <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.4896877], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# random seed 값 지정, global한 randomseed값과 operation seed를 모두 설정해야 같은 결과값을 도출할 수 있음.\n",
    "tf.random.set_seed(7777)\n",
    "\n",
    "# 최대 x 범위 : start <= x < limit, delta로 슬라이싱\n",
    "start, limit, delta = 0, 1, 0.01\n",
    "x = tf.Variable(tf.range(start, limit, delta))\n",
    "\n",
    "# yhat 생성\n",
    "n = 1\n",
    "yhat_w_1, low_loss_1 = yhat_w_loss(x, n, early_stop=1)\n",
    "yhat_1 = y(x, yhat_w_1, n, bias = 1)\n",
    "yhatprime_1 = yprime(x, yhat_w_1, n, bias = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start add EC\n",
      "Epoch : 0, Loss_sum : 843.3024\n",
      "Epoch : 1000, Loss_sum : 8.8078\n",
      "not this seed : 777, break Epoch 1588\n",
      "Epoch : 0, Loss_sum : 82.9891\n",
      "not this seed : 778, break Epoch 219\n",
      "Epoch : 0, Loss_sum : 517.5748\n",
      "not this seed : 779, break Epoch 582\n",
      "Epoch : 0, Loss_sum : 708.5294\n",
      "not this seed : 780, break Epoch 644\n",
      "Epoch : 0, Loss_sum : 661.6945\n",
      "Epoch : 1000, Loss_sum : 1.0112\n",
      "Epoch : 2000, Loss_sum : 0.9833\n",
      "Epoch : 3000, Loss_sum : 0.9653\n",
      "Epoch : 4000, Loss_sum : 0.9367\n",
      "Epoch : 5000, Loss_sum : 0.8925\n",
      "Epoch : 6000, Loss_sum : 0.8263\n",
      "not this seed : 781, break Epoch 6315\n",
      "Epoch : 0, Loss_sum : 882.5511\n",
      "Epoch : 1000, Loss_sum : 1.3438\n",
      "Epoch : 2000, Loss_sum : 0.9483\n",
      "Epoch : 3000, Loss_sum : 0.9186\n",
      "Epoch : 4000, Loss_sum : 0.8731\n",
      "Epoch : 5000, Loss_sum : 0.8067\n",
      "not this seed : 782, break Epoch 5085\n",
      "Epoch : 0, Loss_sum : 623.8611\n",
      "not this seed : 783, break Epoch 260\n",
      "Epoch : 0, Loss_sum : 1020.5853\n",
      "Epoch : 1000, Loss_sum : 4.3088\n",
      "Epoch : 2000, Loss_sum : 1.7180\n",
      "Epoch : 3000, Loss_sum : 1.6437\n",
      "Epoch : 4000, Loss_sum : 1.5321\n",
      "Epoch : 5000, Loss_sum : 1.3752\n",
      "Epoch : 6000, Loss_sum : 1.1797\n",
      "Epoch : 7000, Loss_sum : 0.9631\n",
      "not this seed : 784, break Epoch 7713\n",
      "Epoch : 0, Loss_sum : 754.4098\n",
      "not this seed : 785, break Epoch 470\n",
      "Epoch : 0, Loss_sum : 615.0640\n",
      "not this seed : 786, break Epoch 491\n",
      "Epoch : 0, Loss_sum : 755.3367\n",
      "not this seed : 787, break Epoch 373\n",
      "Epoch : 0, Loss_sum : 525.9670\n",
      "Epoch : 1000, Loss_sum : 0.8448\n",
      "Epoch : 2000, Loss_sum : 0.8354\n",
      "Epoch : 3000, Loss_sum : 0.8225\n",
      "Epoch : 4000, Loss_sum : 0.8019\n",
      "not this seed : 788, break Epoch 4071\n",
      "Epoch : 0, Loss_sum : 175.9682\n",
      "Epoch : 1000, Loss_sum : 1.4855\n",
      "Epoch : 2000, Loss_sum : 1.4481\n",
      "Epoch : 3000, Loss_sum : 1.3881\n"
     ]
    }
   ],
   "source": [
    "# add EC\n",
    "EC_W_1, EC_b_1, loss_1 = EC_W(x, yhat_1, yhatprime_1, n, early_stop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "fig = plt.figure(figsize=(18,5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(yhat_1, label = 'yhat')\n",
    "ax.plot([yhat_1[i] + EC_b_1 + neural_network(EC_W_1, x[i])*x[i]**(n+1) for i in tf.range(tf.size(x))], label = 'y')\n",
    "ax.plot(psy_analytic(x,1), label = 'analysis')\n",
    "ax.tick_params(labelsize=10)\n",
    "ax.legend(fontsize=20)h \n",
    "ax.grid()\n",
    "# Error\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(abs(yhat_1 - psy_analytic(x,1)), label = 'yhat')\n",
    "ax2.plot(abs([yhat_1[i] + EC_b_1 + neural_network(EC_W_1, x[i])*x[i]**(n+1) for i in tf.range(tf.size(x))] - psy_analytic(x,1)), label = 'y')\n",
    "ax2.tick_params(labelsize=10)\n",
    "ax2.legend(fontsize=20)\n",
    "ax2.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ODE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a37a875133dc80c33f2ec96aea7a9fa5375ebe8d8c5e67883faafd638e38fe9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
