{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KyungRyeolBaek/study/blob/main/AI%20study/ODE/ErrorCorr_f(xy)_21_07_25_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x, y)\n",
    "\n",
    "anal(x)\n",
    "\n",
    "Error_Corr\n",
    "    sigmoid(x)\n",
    "\n",
    "    sigmoid_grad(x)\n",
    "\n",
    "    neural_network(W, x)\n",
    "\n",
    "    d_neural_network_dx(W, x, k=1)\n",
    "\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python version 3.10.4\n",
    "\n",
    "tensorflow version 2.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UnreadVariable' object has no attribute 'trainable_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kyungryeol\\github\\ODE\\ODE\\ErrorCorr_f(xy)_22_06_01.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000005?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(count):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000005?line=44'>45</a>\u001b[0m     train\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000005?line=45'>46</a>\u001b[0m train\u001b[39m.\u001b[39;49mtrainable_variables\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_UnreadVariable' object has no attribute 'trainable_variables'"
     ]
    }
   ],
   "source": [
    "# 출력할 x 범위\n",
    "x = tf.Variable(np.array([[i/100] for i in range(0, 100, 1)], dtype = np.float32))\n",
    "\n",
    "# 차수, weight 개수\n",
    "n = 4\n",
    "\n",
    "# w = [weight1, weight2, weight3, weight4]\n",
    "# weight에 쓸 변수 4개를 v에 랜덤변수 n개를 만듭니다.\n",
    "v = tf.Variable(tf.random.truncated_normal([n]))\n",
    "# w를 뽑아낸 랜덤변수 n개를 담고 있는 v로 대체 합니다.\n",
    "w = tf.Variable(v.read_value())\n",
    "\n",
    "# analysis 함수 \n",
    "def anal(x, lamb = 1):\n",
    "    return np.exp(lamb*x)\n",
    "\n",
    "def f(x, y, lamb = 1):\n",
    "    return lamb*x*y\n",
    "\n",
    "# n : w 개수, w : weight\n",
    "def y(x, w, n = 4):\n",
    "    y = 1\n",
    "    \n",
    "    # y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4)\n",
    "    for i in range(1, n+1):\n",
    "        y += w[i-1]*(x**i)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def yprime(x, w, n = 4):\n",
    "    yprime = 0\n",
    "\n",
    "    # yprime = w1 + 2*w2*x + 3*w3*(x**2) + 4*w4*(x**3)\n",
    "    for i in range(1, n + 1):\n",
    "      yprime += i*w[i-1]*(x**(i-1))\n",
    "\n",
    "    return yprime\n",
    "\n",
    "\n",
    "cost = lambda : tf.reduce_mean(tf.square(yprime(x, w, n = 4) - y(x, w, n = 4)))\n",
    "optimizer = tf.optimizers.SGD(learning_rate = 1e-2)\n",
    "train = optimizer.minimize(cost, var_list=w)\n",
    "count = 100\n",
    "for _ in range(count):\n",
    "    train\n",
    "train.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.009570697>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.square(yprime(x, w, n = 4) - y(x, w, n = 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련, 테스트 데이터 생성\n",
    "# analysis 함수를 통해 x : 입력 값, y : 출력 값 을 생성 했습니다.\n",
    "x_data = np.array(x)\n",
    "y_data = anal(x)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(len(X_train) / 10)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(batch_size,1), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = layers.Dense(batch_size, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert the argument `type_value`: <function <lambda> at 0x00000183AE554CA0> to a TensorFlow DType.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kyungryeol\\github\\ODE\\ODE\\ErrorCorr_f(xy)_22_06_01.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000021?line=18'>19</a>\u001b[0m     loss_value \u001b[39m=\u001b[39m cost\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000021?line=20'>21</a>\u001b[0m \u001b[39m# Use the gradient tape to automatically retrieve\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000021?line=21'>22</a>\u001b[0m \u001b[39m# the gradients of the trainable variables with respect to the loss.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000021?line=22'>23</a>\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss_value, model\u001b[39m.\u001b[39;49mtrainable_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000021?line=24'>25</a>\u001b[0m \u001b[39m# Run one step of gradient descent by updating\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000021?line=25'>26</a>\u001b[0m \u001b[39m# the value of the variables to minimize the loss.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kyungryeol/github/ODE/ODE/ErrorCorr_f%28xy%29_22_06_01.ipynb#ch0000021?line=26'>27</a>\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, model\u001b[39m.\u001b[39mtrainable_weights))\n",
      "File \u001b[1;32mc:\\Users\\kyungryeol\\Anaconda3\\envs\\ODE\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1064\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop.py?line=1061'>1062</a>\u001b[0m flat_targets \u001b[39m=\u001b[39m []\n\u001b[0;32m   <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop.py?line=1062'>1063</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(target):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop.py?line=1063'>1064</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m backprop_util\u001b[39m.\u001b[39;49mIsTrainable(t):\n\u001b[0;32m   <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop.py?line=1064'>1065</a>\u001b[0m     logging\u001b[39m.\u001b[39mvlog(\n\u001b[0;32m   <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop.py?line=1065'>1066</a>\u001b[0m         logging\u001b[39m.\u001b[39mWARN, \u001b[39m\"\u001b[39m\u001b[39mThe dtype of the target tensor must be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop.py?line=1066'>1067</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfloating (e.g. tf.float32) when calling GradientTape.gradient, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop.py?line=1067'>1068</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, t\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop.py?line=1068'>1069</a>\u001b[0m   \u001b[39mif\u001b[39;00m resource_variable_ops\u001b[39m.\u001b[39mis_resource_variable(t):\n",
      "File \u001b[1;32mc:\\Users\\kyungryeol\\Anaconda3\\envs\\ODE\\lib\\site-packages\\tensorflow\\python\\eager\\backprop_util.py:54\u001b[0m, in \u001b[0;36mIsTrainable\u001b[1;34m(tensor_or_dtype)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop_util.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop_util.py?line=52'>53</a>\u001b[0m   dtype \u001b[39m=\u001b[39m tensor_or_dtype\n\u001b[1;32m---> <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop_util.py?line=53'>54</a>\u001b[0m dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39;49mas_dtype(dtype)\n\u001b[0;32m     <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop_util.py?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dtype\u001b[39m.\u001b[39mbase_dtype \u001b[39min\u001b[39;00m (dtypes\u001b[39m.\u001b[39mfloat16, dtypes\u001b[39m.\u001b[39mfloat32, dtypes\u001b[39m.\u001b[39mfloat64,\n\u001b[0;32m     <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop_util.py?line=55'>56</a>\u001b[0m                             dtypes\u001b[39m.\u001b[39mcomplex64, dtypes\u001b[39m.\u001b[39mcomplex128,\n\u001b[0;32m     <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/eager/backprop_util.py?line=56'>57</a>\u001b[0m                             dtypes\u001b[39m.\u001b[39mresource, dtypes\u001b[39m.\u001b[39mvariant, dtypes\u001b[39m.\u001b[39mbfloat16)\n",
      "File \u001b[1;32mc:\\Users\\kyungryeol\\Anaconda3\\envs\\ODE\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:721\u001b[0m, in \u001b[0;36mas_dtype\u001b[1;34m(type_value)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/framework/dtypes.py?line=717'>718</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(type_value, _dtypes\u001b[39m.\u001b[39mDType):\n\u001b[0;32m    <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/framework/dtypes.py?line=718'>719</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _INTERN_TABLE[type_value\u001b[39m.\u001b[39mas_datatype_enum]\n\u001b[1;32m--> <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/framework/dtypes.py?line=720'>721</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot convert the argument `type_value`: \u001b[39m\u001b[39m{\u001b[39;00mtype_value\u001b[39m!r}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/kyungryeol/Anaconda3/envs/ODE/lib/site-packages/tensorflow/python/framework/dtypes.py?line=721'>722</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mto a TensorFlow DType.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert the argument `type_value`: <function <lambda> at 0x00000183AE554CA0> to a TensorFlow DType."
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = cost\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google\n",
    "# !pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad \n",
    "import autograd.numpy.random as npr\n",
    "from autograd.core import primitive\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from tensorflow.python.client import device_lib \n",
    "device_lib.list_local_devices() \n",
    "tf.test.is_gpu_available()\n",
    "# gpu를 사용하려면 cmd와 jupyter notebook cell에서 모두 True값이 나와야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = 1             # 최대 x 범위 : 0 <= x < x_max\n",
    "w_number = 6          # w 개수\n",
    "cost_limit = 0.1      # 최소 cost \n",
    "step_number = 30000   # step 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):          # f(x, y)\n",
    "  return 2*x*y\n",
    "\n",
    "def anal(x):\n",
    "  return np.exp(x)\n",
    "\n",
    "def Error_Corr(x_max = 1, w_number = 6, cost_limt = 0.1, step_number = 30000):\n",
    "  def sigmoid(x):\n",
    "      return 1 / (1 + tf.math.exp(-x))\n",
    "\n",
    "  def sigmoid_grad(x):\n",
    "      return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "  def neural_network(W, x):\n",
    "      a1 = sigmoid(tf.tensordot(x, W[0], axes = 1))\n",
    "      return tf.tensordot(a1, W[1], axes = 1)\n",
    "\n",
    "  def d_neural_network_dx(W, x, k=1):\n",
    "      return tf.tensordot(tf.transpose(W[1]), tf.transpose(W[0])**k, axes = 1) * sigmoid_grad(x)\n",
    "\n",
    "  x = tf.Variable(np.array([[i/100] for i in range(0, x_max * 100, 1)], dtype = np.float32))\n",
    "  W = [tf.Variable(tf.random_normal([1, 10])), tf.Variable(tf.random_normal([10, 1]))]\n",
    "  for i in range(1, w_number + 1):\n",
    "      globals()['w{}'.format(i)] = tf.Variable(tf.random_normal([1]), name = 'weight{}'.format(i))\n",
    "  y = 1\n",
    "  yprime = 0\n",
    "\n",
    "  sess = tf.Session()\n",
    "  init = tf.global_variables_initializer()\n",
    "  sess.run(init)\n",
    "\n",
    "  for i in range(1, w_number + 1):\n",
    "      y += globals()['w{}'.format(i)]*(x**i)  # y = 1 + w1*(x) + w2*(x**2) + w3*(x**3) + w4*(x**4) + w5*(x**5) + w6*(x**6)\n",
    "      yprime += i*globals()['w{}'.format(i)]*(x**(i-1))  # yprime = w1 + 2*w2*x + 3*w3*(x**2) + 4*w4*(x**3) + 5*w5*(x**4) + 6*w6*(x**5)\n",
    "\n",
    "  ye = (x**(w_number + 1)) * neural_network(W, x)\n",
    "  yeprime = (w_number + 1) * (x**(w_number))*neural_network(W, x) + (x**(w_number + 1)) * d_neural_network_dx(W, x)\n",
    "\n",
    "  cost = tf.reduce_mean(tf.square(yeprime - ye)) + tf.reduce_mean(tf.square(yprime - f(x, y)))\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "  train = optimizer.minimize(cost)\n",
    "\n",
    "  count = 0\n",
    "  cost_val = 1\n",
    "  while cost_val > cost_limt:\n",
    "    global x_space\n",
    "    global y_space\n",
    "    global ye_space\n",
    "    global corr_space\n",
    "    global cost_space\n",
    "\n",
    "    x_space = []\n",
    "    y_space = []\n",
    "    ye_space = []\n",
    "    corr_space = []\n",
    "    cost_space = []\n",
    "\n",
    "    if count % 1 == 0:\n",
    "      print(\"count: \", count)\n",
    "    for step in range(step_number + 1):\n",
    "      for i in range(1, w_number + 1):\n",
    "          globals()['w{}_val'.format(i)] = sess.run([globals()['w{}'.format(i)]])\n",
    "      cost_val, yeprime_val, ye_val, yprime_val, y_val, x_val, _ = sess.run([cost, yeprime, ye, yprime, f(x, y), x, train])\n",
    "      if count % 1 == 0:\n",
    "        if step % int(step_number / 2) == 0:\n",
    "            print('step: ', step, \", Cost: \", cost_val, \"\\n\")\n",
    "    if count % 1 == 0:\n",
    "      print(\"\\n\")\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    x_space += list(x_val)\n",
    "    corr_space += list(ye_val + y_val)\n",
    "    y_space += list(y_val)\n",
    "    ye_space += list(ye_val)\n",
    "\n",
    "    # final_sol = ye + f(x, y)\n",
    "    # anal_sol = anal(x)\n",
    "    # error = abs(final_sol - anal_sol)\n",
    "    # print(np.linalg.norm)\n",
    "    plt.plot(x_space, corr_space, '.', label = 'ye + f(x,y)')\n",
    "    plt.plot(x_space, y_space, '.', label = 'f(x,y)')\n",
    "    plt.plot(x_space, ye_space, '.', label = 'ye')\n",
    "    plt.plot(x_space, anal(x_space), '.', label = 'exp')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "  sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = 1             # 최대 x 범위 : 0 <= x < x_max\n",
    "w_number = 6          # w 개수\n",
    "cost_limit = 0.01      # 최소 cost \n",
    "step_number = 50    # step 횟수 -> 총 step 횟수 = count * step\n",
    "\n",
    "def f(x, y):          # f(x, y)\n",
    "  return y\n",
    "\n",
    "def anal(x):\n",
    "  return np.exp(x)\n",
    "\n",
    "Error_Corr(x_max, w_number, cost_limit, step_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7003669\n"
     ]
    }
   ],
   "source": [
    "error = abs(corr_space - anal(x_space))\n",
    "plt.plot(x_space, anal(x_space), '.', label = 'anal sol')\n",
    "plt.plot(x_space, corr_space, '.', label = 'final sol')\n",
    "plt.plot(x_space, error, '.', label = 'error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(np.linalg.norm(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMwtPl14hsTGhfTXgC6Fsdr",
   "include_colab_link": true,
   "name": "Untitled13.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a37a875133dc80c33f2ec96aea7a9fa5375ebe8d8c5e67883faafd638e38fe9a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ODE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
